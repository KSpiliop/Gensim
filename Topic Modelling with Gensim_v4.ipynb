{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling with Gensim : the LDA algorithm\n",
    "\n",
    "LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
    "\n",
    "### 1. Loading of the required libraries and vocabularies.\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "## Run once this chunk with the command below active. Then comment it out and run the notebook\n",
    "!{sys.executable} -m spacy download en\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "##nltk.download('stopwords')\n",
    "##stop_words = stopwords.words('english')\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Pre-processing\n",
    "\n",
    "* Read two files exported from the database which contain the SE Glossary articles definitions and their titles.\n",
    "* In later versions, the **corresponding tables will be directly exported from the database**.\n",
    "* Merge by_id_ and discard records with duplicate titles and/or definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1= pd.read_csv(\"ESTAT_dat_concepts_2021_04_08.csv\",sep=\";\")\n",
    "dat2= pd.read_csv(\"ESTAT_dat_link_info_2021_04_08.csv\",sep=\";\")\n",
    "Gloss_concepts = pd.merge(dat1,dat2,on=['id'])\n",
    "del(dat1,dat2)\n",
    "\n",
    "Gloss_concepts = Gloss_concepts[['id','title','definition']]\n",
    "\n",
    "Gloss_concepts = Gloss_concepts.drop_duplicates(subset=[\"definition\"])\n",
    "Gloss_concepts = Gloss_concepts.dropna(axis=0,subset=[\"definition\"])\n",
    "Gloss_concepts = Gloss_concepts.drop_duplicates(subset=[\"title\"])\n",
    "Gloss_concepts = Gloss_concepts.dropna(axis=0,subset=[\"title\"])\n",
    "\n",
    "Gloss_concepts.reset_index(drop=True, inplace=True)\n",
    "Gloss_concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-processing input data (cont).\n",
    "***\n",
    "\n",
    "Next we tokenize the texts - definitions in the articles, select tokens with minimum length 5, delete stop words and apply a simple pre-processing (convert to lowercase, drop accents). \n",
    "\n",
    "The result, _texts_ is a list with 1286 elements corresponding to the records in the dataframe _df_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize texts and clean-up text.\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_stopwords(sentence) ## remove stop words\n",
    "        tokens = gensim.utils.tokenize(sentence)\n",
    "        sentence = [token for token in tokens if len(token) >= 5] ##minimum length = 5 \n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # to lower + deacc=True removes punctuations\n",
    "        \n",
    "texts = list(sent_to_words(Gloss_concepts['definition']))\n",
    "print('\\nFirst 10 texts: \\n',texts[:10])\n",
    "print('\\nTotal texts: ',len(texts),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creation of corpus and terms frequencies.\n",
    "***\n",
    "\n",
    "Next we create:\n",
    "* a dictionary from _texts_ with name _id2word_. This has 7258 unique tokens. \n",
    "* a mapping with name _corpus_ of the texts into lists with tuples: (word id, frequency in each text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary\n",
    "id2word = corpora.Dictionary(texts) #Gensim creates a unique id for each word in the document. \n",
    "print(id2word,'\\n')\n",
    "\n",
    "## The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "#Alternatively:\n",
    "corpus = [id2word.doc2bow(text) for text in texts] #corpus package automatically creates a set of corpus reader instances that can be used to access the corpora in the NLTK data package.\n",
    "\n",
    "print('First 10 texts:\\n')\n",
    "print(corpus[:10])\n",
    "print('\\nTotal texts: ',len(corpus))\n",
    "\n",
    "#Human readable format of corpus (term-frequency)\n",
    "#[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dominant topic in each document.\n",
    "***\n",
    "Î‘t this stage we build the  _lda_model_, through which the dominant topic for each document will be extracted and we show the weight of the topic and the keywords. We define a dataframe _df_topic_sents_keywords_  in order to store these dominant topics. \n",
    "\n",
    "In function _format_topics_sentences()_ below, the list _ldamodel[corpus]_ has one nested list element per text. Each nested list contains tuples (topic, contribution). We sort each nested list by descending contribution to find the dominant topic and then, we retrieve for this topic, the list _wp_ of tuples (word, probability) for the most probable words, using the function _ldamodel.show_topic()_. We join these words into a list and put the result in column 'Topic_Keywords' of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the Dominant topic and its percentage contribution in each document.\n",
    "\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]): \n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list ## ldamodel.per_word_topics is False          \n",
    "        #print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True) ## sort the nested list by descending contribution\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        topic_num, prop_topic = row[0]\n",
    "        wp = ldamodel.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original tokenized text and title to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, Gloss_concepts['id'],Gloss_concepts['title']], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus)\n",
    "df_topic_sents_keywords.rename(columns = {0:'Text tokenized definition'}, inplace = True)\n",
    "df_topic_sents_keywords.rename(columns = {'id':'Text id','title':'Text title'}, inplace = True)\n",
    "df_topic_sents_keywords = df_topic_sents_keywords[['Text id','Text title','Text tokenized definition','Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']]\n",
    "df_topic_sents_keywords['Dominant_Topic'] = df_topic_sents_keywords['Dominant_Topic'].astype(int)\n",
    "df_topic_sents_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Most representative document for each topic.\n",
    "***\n",
    "\n",
    "**Need to add the document id**\n",
    "\n",
    "We want to take samples of documents that best represent a given topic, to have a complete analysis.This code receives the most exemplary document for each topic. We create the _sent_topics_sorteddf_mallet_ and these Topic Keywords are presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords[['Text id','Text title','Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']].groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most representative document for each topic\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Text id','Text title','Topic_Num', 'Topic_Perc_Contrib', 'Topic Keywords' ]\n",
    "sent_topics_sorteddf_mallet = sent_topics_sorteddf_mallet[['Topic_Num', 'Topic_Perc_Contrib', 'Topic Keywords','Text id','Text title']]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The topics as a mix of keywords.\n",
    "***\n",
    "\n",
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes with a certain weight to the topic.\n",
    "Finally,we want to understand the volume and distribution of topics in order to judge how widely it was discussed,so we define the _df_dominant_topics_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Keyword in the 20 topics\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualization of the topics.\n",
    "***\n",
    "\n",
    "To visualize the fitted LDA model we use the _pyLDAvis_ package. This is the Python porting of the R package _LDAvis_, see [LDAvis vignette](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf) for details and Chuang, Jason, Manning, Christopher D., and Heer, Jeffrey (2012). Termite: Visualization Techniques for Assessing Textual Topic Models, *Advanced Visual Interfaces* for the theory behind the visualization algorithm. The paper is available [here](https://dl.acm.org/doi/pdf/10.1145/2254556.2254572?casa_token=q2BavKP415QAAAAA:MhcYHzz4PJpC7dNkkm12GL-ohQRUXBgumPJ9l1t_5n3M4qVE1kdDqKGfPmtnR7qbale_ukS-2nJs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the topics\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic distribution across documents\n",
    "\n",
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. The model's coherence score.\n",
    "***\n",
    "\n",
    "This is based on the work in RÃ¶der, M., Both, A., & Hinneburg, A. (2015, February). Exploring the space of topic coherence measures. In *Proceedings of the eighth ACM international conference on Web search and data mining* (pp. 399-408), available [here](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_topics = lda_model.get_document_topics(corpus,minimum_probability=0.3)\n",
    "res  = [d for d in docs_topics]\n",
    "res\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lda_model[corpus]))\n",
    "[a for a in lda_model[corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
